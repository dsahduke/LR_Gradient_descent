{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In supervised machine learning, we have some data, $X$ and some labels $y$ associated with $X$. The goal is learn a mapping f(X) -> y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If y is a continuous variable, this type of procedure is known as regression. However, if y takes on discrete values, such as 1 or 0, or one of $k$ values, this procedure is known as classification. In the special case where there are only 2 states that y can take, this is known as binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In many cases, we'd like y to represent the probability of an event occuring. We could use linear regression to do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/notlinear.png)\n",
    "ISL: http://www-bcf.usc.edu/~gareth/ISL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Logistic regression is one of the most popular binary classification algorithms and is related to linear regression. It belongs to a class of models known as *generalized linear models* or GLMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Logistic Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before we see the full model specification for logistic regression, let's first take a look at the logistic function, from which logistic regression derives its name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\sigma(x) = \\frac{e^x}{1+e^x}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that this is also equivalent to the following:\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "y = np.exp(x)/(1+np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y)\n",
    "plt.title('Logistic Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\sigma(x)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ P(y=1|x) = \\frac{e^{\\textbf{w}^T \\textbf{x}}}{1 + e^{\\textbf{w}^T\\textbf{x}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ P(y = 0 | x) = 1 - \\frac{e^{\\textbf{w}^T \\textbf{x}}}{1 + e^{\\textbf{w}^T\\textbf{x}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ =  \\frac{1 + e^{\\textbf{w}^T\\textbf{x}}}{1 + e^{\\textbf{w}^T\\textbf{x}}} - \\frac{e^{\\textbf{w}^T \\textbf{x}}}{1 + e^{\\textbf{w}^T\\textbf{x}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$  = \\frac{1}{1+e^{\\textbf{w}^T\\textbf{x}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\frac{p(y=1|x)}{p(y=0|x)} = e^{\\textbf{w}^T\\textbf{x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The term \n",
    "$$ \\frac{p(y =1|x)}{p(y=0|x)} $$ or more generally $$ \\frac{p}{1-p}$$  is known as *odds*. Odds can take values between 0 and $\\infty$. This is used predominantly in betting. For example, a probability of 0.2 corresponds to odds of 1:4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we take the logarithm of both sides, we're left with \n",
    "$$ \\log \\frac{p}{1-p} = \\textbf{w}^T\\textbf{x} $$\n",
    "\n",
    "which looks like linear regression!\n",
    "\n",
    "The right hand side, in statistical notation, is \n",
    "\n",
    "$$ \\beta_0 + \\beta_1x_1 + \\beta_2x_2 ... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretation of coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Odds Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ y_1 = e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_2 = e^{\\beta_0 + \\beta_1(x_1+1) + \\beta_2x2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{y_2}{y_1} = \\frac{e^{\\beta_0 + \\beta_1(x_1+1) + \\beta_2x_2}}{e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\frac{y_2}{y_1} = e^{\\beta_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where $\\frac{y_2}{y_1}$ is the *odds ratio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### log Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\log \\frac{p}{1-p} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we estimate the weights, _w_ or coefficients, $\\beta$? Minimize the loss! (Or equivalently, maximize the likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As a reminder, the gradient, $\\nabla$, is defined as a vector of the partial derivatives of a function with respect to the dimensions in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Specifically, we are interested in the gradient of the loss function with respect to each of the parameters of the model. For example, if we have a loss function:\n",
    "\n",
    "$$ l(w) $$\n",
    "\n",
    "and a model $$ y = w_0 + w_1x_1 + w_2x_2$$\n",
    "The gradient $\\nabla l$ can be written as\n",
    "\n",
    "$$ \\begin{bmatrix} \\frac{\\partial l}{\\partial w_0} \\\\ \\frac{\\partial l}{\\partial w_1} \\\\ \\frac{\\partial l}{\\partial w_2} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here, we introduce the algorithm known as Gradient Descent/Gradient Ascent. The gradient \n",
    "\n",
    "$$ \\nabla F(\\textbf{x})$$ points in the direction of steepest ascent for F. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/steepest.png)\n",
    "Bamshad Mabasher\n",
    "DePaul University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/steepest_ascent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ P(y=1|X) = \\frac{1}{1+e^{-\\textbf{w}^T\\textbf{x}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here, $\\textbf{w}$ is made up of a bias term, $b$ and weights $w_i$ from $i = 1...j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For logistic regression, we use the *cross-entropy* loss, which is given by \n",
    "\n",
    "$$loss =  -y \\log \\sigma(\\textbf{w}^T\\textbf{x}) + (1-y) \\log (1 - \\sigma(\\textbf{w}^T\\textbf{x})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What we would like to do is to get the average loss over all y's and x's. Here, I've omitted the negative sign out front, to turn this into a likelihood maximization problem. So we get the following expression:\n",
    "\n",
    "$$\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)} \\log \\sigma(\\textbf{w}^T\\textbf{x}^{(i)}) + (1-y^{(i)}) \\log (1- \\sigma(\\textbf{w}^T\\textbf{x}^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "for $i = 1...m$ training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we need to take the gradient of this function with respect to each of our weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we do that, here is a convenient mathematical trick:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}} = (1+e^{-x})^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{d}{dx}\\sigma(x) = -(1+e^{-x})^{-2}(-e^{-x})$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ = \\frac{e^{-x}}{(1+e^{-x})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ = \\frac{1}{1+e^{-x}} * \\frac{e^{-x}}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ = \\frac{1}{1+e^{-x}}\\big(\\frac{1+e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}\\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\sigma'(x)= \\sigma(x)(1-\\sigma(x)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Partial derivatives of the loss function with respect to weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we want to find the derivative of the loss function,\n",
    "\n",
    "$$l(w) = y^{(i)} \\log \\sigma(\\textbf{w}^T\\textbf{x}^{(i)}) + (1 - y^{(i)}) \\log (1 - \\sigma(\\textbf{w}^T\\textbf{x}^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "with respect to the weights w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's say that $\\textbf{x}$ is $p$-dimensional, indexed by $j = 1...p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have:\n",
    "    \n",
    "$$ \\frac{\\partial}{\\partial{w_j}}l(w) = \\frac{y}{\\sigma(\\textbf{w}^T\\textbf{x})} \\frac{\\partial}{\\partial{w_j}}\\sigma(\\textbf{w}^T\\textbf{x}) + \\frac{1-y}{(1-\\sigma(\\textbf{w}^T\\textbf{x}))} \\frac{\\partial}{\\partial{w_j}}(1 -\\sigma(\\textbf{w}^T\\textbf{x}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Using the chain rule, and the fact that $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ = \\frac{y}{\\sigma(\\textbf{w}^T\\textbf{x})} \\sigma(\\textbf{w}^T\\textbf{x})(1-\\sigma(\\textbf{w}^T\\textbf{x}))x_j - \\frac{1-y}{(1-\\sigma(\\textbf{w}^T\\textbf{x}))} \\sigma(\\textbf{w}^T\\textbf{x})(1-\\sigma(\\textbf{w}^T\\textbf{x}))x_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ = y (1 - \\sigma(\\textbf{w}^T\\textbf{x}))x_j - (1-y)\\sigma(\\textbf{w}^T\\textbf{x})x_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ = (y - y\\sigma(\\textbf{w}^T\\textbf{x}) - \\sigma(\\textbf{w}^T\\textbf{x}) + y\\sigma(\\textbf{w}^T\\textbf{x}))x_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ = (y - \\sigma(\\textbf{w}^T\\textbf{x}))x_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take a step back and realize what this means. The partial derivative of the loss function with respect to any of the weights, is just the actual predictive error multiplied by the magnitude of the x value that corresponds with the weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Ascent/Descent update equation: \n",
    "\n",
    "We want to move in the direction of the gradient (if we are maximizing the likelihood -- known as gradient ascent) or going against the direction of the gradient (minimizing loss -- gradient descent)\n",
    "\n",
    "For Gradient Ascent:\n",
    "\n",
    "$$ w_j \\leftarrow w_j + \\alpha(y-\\sigma(\\textbf{w}^T\\textbf{x}))x_j $$\n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "$$ w_j \\leftarrow w_j - \\alpha(\\sigma(\\textbf{w}^T\\textbf{x} - y))x_j $$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice that in the update equations above, there is an $\\alpha$ term. This is sometimes also shown as $\\eta$. Essentially, it describes the *step size* that we take in the direction of the gradient. If this number is too high, it can be possible to overshoot the minimum/maximum, and if it is too low, it may take a long time to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/learningrate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variants of Gradient Descent/Ascent\n",
    "http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The standard Gradient Descent approach is known as Bach Gradient Descent, or just Gradient Descent. We compute the average gradient over all weights over the entire dataset, and then apply the update equation once. This has some advantages and disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Advantages:\n",
    "    * Always head in the right direction\n",
    "Disadvantages:\n",
    "    * Not efficient in memory or in computation (have to compute over whole dataset)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In stochastic gradient descent, we simply shuffle all of our training examples, and then iterate 1 at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ w_j \\leftarrow w_j + \\alpha(y-\\sigma(\\textbf{w}^T\\textbf{x}))x_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "gets applied for every single training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is a noisy operation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/batchsgd.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
